# LiteAVSE Configuration
# Lightweight Audio-Visual Speech Enhancement using Causal Mamba

# Environment Settings
env_setting:
  num_gpus: 1
  num_workers: 4
  seed: 1234
  stdout_interval: 10
  checkpoint_interval: 500
  validation_interval: 500
  summary_interval: 50
  dist_cfg:
    dist_backend: nccl
    dist_url: tcp://localhost:19499
    world_size: 1

# Datapath Configuration
data_cfg:
  train_data_json: data/grid_train.json
  valid_data_json: data/grid_valid.json
  test_data_json: data/grid_test.json
  train_noise_json: data/noise_demand.json
  valid_noise_json: data/noise_demand.json

# Training Configuration
training_cfg:
  training_epochs: 100
  batch_size: 2
  learning_rate: 0.0005
  adam_b1: 0.8
  adam_b2: 0.99
  lr_decay: 0.99
  segment_size: 16000    # 1 second segments at 16kHz
  snr_range: [-5, 20]
  use_discriminator: False
  loss:
    magnitude: 0.9
    phase: 0.3
    complex: 0.1
    time: 0.2
    consistancy: 0.1
    si_sdr: 0.2
  use_PCS400: False

# STFT Configuration
stft_cfg:
  sampling_rate: 16000
  n_fft: 400
  hop_size: 100
  win_size: 400

# Model Configuration
model_cfg:
  hid_feature: 64
  compress_factor: 0.3
  num_tfmamba: 4
  d_state: 16
  d_conv: 4
  expand: 4
  norm_epsilon: 0.00001
  beta: 2.0
  input_channel: 2   # Audio only, visual fuses in feature space via VCE+FSVG
  output_channel: 1

# Visual Configuration
visual_cfg:
  use_visual: True
  visual_channels: 1
  freeze_visual_encoder: True
  video_fps: 25
  face_size: 96

# LiteAVSE-specific Configuration
lite_cfg:
  visual_encoder_type: mobilenet   # mobilenet EncoderA or custom3d EncoderB
  n_freq_enc: 100                  # F_enc = (n_fft//2 + 1 - 3) / 2 + 1 = 100
  use_lite_dense: False            # True = use depthwise separable conv in DenseBlock, smaller model
